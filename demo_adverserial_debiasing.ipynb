{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from tabulate import tabulate\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from adversarial_debiasing import AdversarialDebiasing\n",
    "from load_data import load_data, transform_data, Datapoint\n",
    "\n",
    "from load_vectors import load_pretrained_vectors, load_vectors\n",
    "import config\n",
    "import utility_functions\n",
    "\n",
    "import gensim\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For autoreloading changes made in other python scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD2VEC_FILE = \"data/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.GzipFile(fileobj=open(WORD2VEC_FILE, \"rb\", buffering=0)) as f:\n",
    "    word_vectors = utility_functions.load_word2vec_format(f, max_num_words=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the word vectors dictionary\n",
    "# word_vectors = load_pretrained_vectors(config.wiki_embedding_data_path, config.save_dir, config.wiki_save_file, config.use_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the google analogies training dataset:\n",
    "analogy_dataset = load_data()\n",
    "analogy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data such that it includes the embeddings of the words in consideration\n",
    "transformed_analogy_dataset, gender_subspace = transform_data(word_vectors, analogy_dataset, use_boluk = False)\n",
    "word_embedding_dim = transformed_analogy_dataset[0].gt_embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the transformed analogy dataset\n",
    "print(transformed_analogy_dataset[0].analogy_embeddings.shape)\n",
    "print(transformed_analogy_dataset[0].gt_embedding.shape)\n",
    "print(transformed_analogy_dataset[0].protected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the grid-search and obtain the np.dot(w.T, g) values\n",
    "learning_rate_list = [2 ** -12, 2 ** -6, 2 ** -3]\n",
    "adversary_loss_weight_list = [1.0, 0.5, 0.1]\n",
    "embedding_types = [\"GoogleNews\", \"WikipediaVec\", \"Glove\"]\n",
    "for embedding_type in embeddings_types:\n",
    "    word_vectors = load_pretrained_vectors(path, save_dir, embedding_type)\n",
    "    transformed_analogy_dataset, gender_subspace = transform_data(word_vectors, analogy_dataset, use_boluk = False)\n",
    "    word_embedding_dim = transformed_analogy_dataset[0].gt_embedding.shape[0]\n",
    "    for learning_rate in learning_rate_list:\n",
    "        for adversary_loss_weight in adversary_loss_weight_list:\n",
    "\n",
    "            non_debiased_model = AdversarialDebiasing(\n",
    "                word_embedding_dim = word_embedding_dim,\n",
    "                num_epochs = 500,\n",
    "                debias = False,\n",
    "                gender_subspace = gender_subspace,\n",
    "                batch_size = 256,\n",
    "                adversary_loss_weight = adversary_loss_weight,\n",
    "                classifier_learning_rate = learning_rate,\n",
    "                adversary_learning_rate = learning_rate\n",
    "            )\n",
    "\n",
    "            non_debiased_model.fit(dataset=transformed_analogy_dataset)\n",
    "\n",
    "            temp_dict = {\"W1\" : non_debiased_model.best_W1, \"W2\" : non_debiased_model.best_W2}\n",
    "\n",
    "            pickle.dump(temp_dict, os.path.join('models', \"non_debiased\", word_embedding_type, str(learning_rate), str(adversary_loss_weight), '.pckl'))\n",
    "\n",
    "            debiased_model = AdversarialDebiasing(\n",
    "                word_embedding_dim = word_embedding_dim,\n",
    "                num_epochs = 500,\n",
    "                debias = True,\n",
    "                gender_subspace = gender_subspace,\n",
    "                batch_size = 256,\n",
    "                adversary_loss_weight = adversary_loss_weight,\n",
    "                classifier_learning_rate = learning_rate,\n",
    "                adversary_learning_rate = learning_rate\n",
    "            )\n",
    "\n",
    "            debiased_model.fit(dataset=transformed_analogy_dataset)\n",
    "\n",
    "            temp_dict = {\"W1\" : debiased_model.best_W1, \"W2\" : debiased_model.best_W2}\n",
    "\n",
    "            pickle.dump(temp_dict, os.path.join('models', \"debiased\", word_embedding_type, str(learning_rate), str(adversary_loss_weight), '.pckl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dim = transformed_analogy_dataset[0].gt_embedding.shape[0]\n",
    "# Training the variant of the model without debiasing\n",
    "non_debiased_model = AdversarialDebiasing(\n",
    "    word_embedding_dim=word_embedding_dim,\n",
    "    num_epochs=500,\n",
    "    debias=False,\n",
    "    gender_subspace=gender_subspace,\n",
    "    batch_size=256,\n",
    "    adversary_loss_weight=0.1,\n",
    "    classifier_learning_rate = 2 ** -6,\n",
    "    adversary_learning_rate = 2 ** -6\n",
    ")\n",
    "non_debiased_model.fit(dataset=transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = non_debiased_model.get_model_weights()\n",
    "print(np.dot(W1.detach().numpy().T,gender_subspace.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_subspace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the variant of the model with debiasing\n",
    "# debiased_model = AdversarialDebiasing(\n",
    "#     word_embedding_dim=word_embedding_dim,\n",
    "#     num_epochs=500,\n",
    "#     debias=True,\n",
    "#     gender_subspace=gender_subspace,\n",
    "#     batch_size=256,\n",
    "#     adversary_loss_weight=0.1,\n",
    "#     classifier_learning_rate = 2 ** -8,\n",
    "#     adversary_learning_rate = 2 ** -8\n",
    "# )\n",
    "debiased_model = AdversarialDebiasing(\n",
    "    word_embedding_dim=word_embedding_dim,\n",
    "    num_epochs=500,\n",
    "    debias=True,\n",
    "    gender_subspace=gender_subspace,\n",
    "    batch_size=256,\n",
    "    adversary_loss_weight=0.1,\n",
    "    classifier_learning_rate = 2 ** -6,\n",
    "    adversary_learning_rate = 2 ** -6\n",
    ")\n",
    "\n",
    "debiased_model.fit(dataset=transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = debiased_model.get_model_weights()\n",
    "print(np.dot(W1.clone().cpu().detach().numpy().T,gender_subspace.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples to test the models upon\n",
    "datapoints, test_analogies = [], []\n",
    "with open(os.path.join('data', 'sexism-traps.txt'), 'r') as f:\n",
    "    # Reading each line\n",
    "    for line in f.readlines():\n",
    "        words = line.split()\n",
    "        if words[0] == ':':\n",
    "            continue\n",
    "        test_analogies.append(words)\n",
    "        word_embeddings = word_vectors[words]\n",
    "        word_embeddings = np.reshape(word_embeddings, (1, -1))\n",
    "        datapoints.append(word_embeddings)\n",
    "datapoints = np.vstack(datapoints)\n",
    "print(datapoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation of the non-debiased model\n",
    "\n",
    "# non_debiased_predictions = non_debiased_model.predict(datapoints)\n",
    "features = torch.cat([torch.Tensor(x).unsqueeze_(0) for x in datapoints])\n",
    "x1 = features[:, 0:word_embedding_dim]\n",
    "x2 = features[:, word_embedding_dim:word_embedding_dim * 2]\n",
    "x3 = features[:, word_embedding_dim * 2:word_embedding_dim * 3]\n",
    "\n",
    "non_debiased_predictions = x2 + x3 - x1\n",
    "non_debiased_predictions = non_debiased_predictions.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_debiased_most_similar_list = utility_functions.obtain_most_similar(non_debiased_predictions, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the similarity list for the non-debiased model\n",
    "non_debiased_most_similar_list_data_frames = []\n",
    "for i in range(len(non_debiased_most_similar_list)):\n",
    "    # print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    temp_data_frame = pd.DataFrame(non_debiased_most_similar_list[i][1:], columns = ['Neighbor', 'Similarity'])\n",
    "    non_debiased_most_similar_list_data_frames.append(temp_data_frame)\n",
    "    # print(tabulate(temp_data_frame, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation of the debiased model\n",
    "debiased_predictions = debiased_model.predict(datapoints)\n",
    "debiased_most_similar_list = utility_functions.obtain_most_similar(debiased_predictions, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the similarity list for the debiased model\n",
    "debiased_most_similar_list_data_frames = []\n",
    "for i in range(len(debiased_most_similar_list)):\n",
    "    # print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    temp_data_frame = pd.DataFrame(debiased_most_similar_list[i][1:], columns = ['Neighbor', 'Similarity'])\n",
    "    debiased_most_similar_list_data_frames.append(temp_data_frame)\n",
    "    # print(tabulate(temp_data_frame, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the dataframes pertaining to both the variants of the model\n",
    "iterables = [['Biased', 'Debiased'], ['Neighbour', 'Similarity']]\n",
    "index = pd.MultiIndex.from_product(iterables)\n",
    "overall_data_frames_list = []\n",
    "for i in range(len(non_debiased_most_similar_list)):\n",
    "    overall_list = []\n",
    "    print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    for j in range(1, len(non_debiased_most_similar_list[i])):\n",
    "        temp_list = []\n",
    "        temp_list.append(non_debiased_most_similar_list[i][j][0])\n",
    "        temp_list.append(round(non_debiased_most_similar_list[i][j][1], 3))\n",
    "        temp_list.append(debiased_most_similar_list[i][j][0])\n",
    "        temp_list.append(round(debiased_most_similar_list[i][j][1], 3))\n",
    "        overall_list.append(temp_list)\n",
    "    temp_df = pd.DataFrame(overall_list, columns = index)\n",
    "    # print(temp_df.to_string(index = False))\n",
    "    print(tabulate(temp_df, headers = ['Biased\\nNeighbour', 'Biased\\nSimilarity', 'Debiased\\nNeighbour', 'Debiased\\nSimilarity'], tablefmt = 'psql', showindex = False))\n",
    "    overall_data_frames_list.append(temp_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondab37d419580d64adfa435fa6a50f93de5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}