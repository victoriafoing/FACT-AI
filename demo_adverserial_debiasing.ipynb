{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from tabulate import tabulate\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from adversarial_debiasing import AdversarialDebiasing\n",
    "from load_data import load_data, transform_data, Datapoint\n",
    "\n",
    "from load_vectors import load_pretrained_vectors, load_vectors\n",
    "import config\n",
    "import utility_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_extautoreload\n"
    }
   ],
   "source": [
    "# For autoreloading changes made in other python scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loading from saved file.\n"
    }
   ],
   "source": [
    "# Loading the word vectors dictionary\n",
    "word_vectors = load_pretrained_vectors(config.glove_embedding_data_path, config.save_dir, config.glove_save_file, config.use_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Banjul', y='Gambia', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Beijing', y='China', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Beirut', y='Lebanon', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Belgrade', y='Serbia', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Belmopan', y='Belize', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Berlin', y='Germany', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Bern', y='Switzerland', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Bishkek', y='Kyrgyzstan', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Bratislava', y='Slovakia', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Brussels', y='Belgium', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Bucharest', y='Romania', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Budapest', y='Hungary', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Bujumbura', y='Burundi', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Cairo', y='Egypt', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Canberra', y='Australia', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Caracas', y='Venezuela', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Chisinau', y='Moldova', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Conakry', y='Guinea', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Copenhagen', y='Denmark', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Dakar', y='Senegal', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Damascus', y='Syria', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Dhaka', y='Bangladesh', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Doha', y='Qatar', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Dublin', y='Ireland', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Dushanbe', y='Tajikistan', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Funafuti', y='Tuvalu', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Gaborone', y='Botswana', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Georgetown', y='Guyana', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Hanoi', y='Vietnam', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Harare', y='Zimbabwe', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Havana', y='Cuba', task='capital-world'),\n Raw_Datapoint(x1='Ashgabat', x2='Turkmenistan', x3='Helsinki', y='Finland', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Astana', y='Kazakhstan', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Athens', y='Greece', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Baghdad', y='Iraq', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Baku', y='Azerbaijan', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Bamako', y='Mali', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Bangkok', y='Thailand', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Banjul', y='Gambia', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Beijing', y='China', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Beirut', y='Lebanon', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Belgrade', y='Serbia', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Belmopan', y='Belize', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Berlin', y='Germany', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Bern', y='Switzerland', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Bishkek', y='Kyrgyzstan', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Bratislava', y='Slovakia', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Brussels', y='Belgium', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Bucharest', y='Romania', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Budapest', y='Hungary', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Bujumbura', y='Burundi', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Cairo', y='Egypt', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Canberra', y='Australia', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Caracas', y='Venezuela', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Chisinau', y='Moldova', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Conakry', y='Guinea', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Copenhagen', y='Denmark', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Dakar', y='Senegal', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Damascus', y='Syria', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Dhaka', y='Bangladesh', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Doha', y='Qatar', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Dublin', y='Ireland', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Dushanbe', y='Tajikistan', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Funafuti', y='Tuvalu', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Gaborone', y='Botswana', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Georgetown', y='Guyana', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Hanoi', y='Vietnam', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Harare', y='Zimbabwe', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Havana', y='Cuba', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Helsinki', y='Finland', task='capital-world'),\n Raw_Datapoint(x1='Asmara', x2='Eritrea', x3='Islamabad', y='Pakistan', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Athens', y='Greece', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Baghdad', y='Iraq', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Baku', y='Azerbaijan', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Bamako', y='Mali', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Bangkok', y='Thailand', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Banjul', y='Gambia', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Beijing', y='China', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Beirut', y='Lebanon', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Belgrade', y='Serbia', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Belmopan', y='Belize', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Berlin', y='Germany', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Bern', y='Switzerland', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Bishkek', y='Kyrgyzstan', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Bratislava', y='Slovakia', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Brussels', y='Belgium', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Bucharest', y='Romania', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Budapest', y='Hungary', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Bujumbura', y='Burundi', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Cairo', y='Egypt', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Canberra', y='Australia', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Caracas', y='Venezuela', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Chisinau', y='Moldova', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Conakry', y='Guinea', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Copenhagen', y='Denmark', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Dakar', y='Senegal', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Damascus', y='Syria', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Dhaka', y='Bangladesh', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Doha', y='Qatar', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Dublin', y='Ireland', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Dushanbe', y='Tajikistan', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Funafuti', y='Tuvalu', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Gaborone', y='Botswana', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Georgetown', y='Guyana', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Hanoi', y='Vietnam', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Harare', y='Zimbabwe', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Havana', y='Cuba', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Helsinki', y='Finland', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Islamabad', y='Pakistan', task='capital-world'),\n Raw_Datapoint(x1='Astana', x2='Kazakhstan', x3='Jakarta', y='Indonesia', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Baghdad', y='Iraq', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Baku', y='Azerbaijan', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bamako', y='Mali', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bangkok', y='Thailand', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Banjul', y='Gambia', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Beijing', y='China', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Beirut', y='Lebanon', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Belgrade', y='Serbia', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Belmopan', y='Belize', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Berlin', y='Germany', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bern', y='Switzerland', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bishkek', y='Kyrgyzstan', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bratislava', y='Slovakia', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Brussels', y='Belgium', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bucharest', y='Romania', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Budapest', y='Hungary', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bujumbura', y='Burundi', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Cairo', y='Egypt', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Canberra', y='Australia', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Caracas', y='Venezuela', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Chisinau', y='Moldova', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Conakry', y='Guinea', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Copenhagen', y='Denmark', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Dakar', y='Senegal', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Damascus', y='Syria', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Dhaka', y='Bangladesh', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Doha', y='Qatar', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Dublin', y='Ireland', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Dushanbe', y='Tajikistan', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Funafuti', y='Tuvalu', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Gaborone', y='Botswana', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Georgetown', y='Guyana', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Hanoi', y='Vietnam', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Harare', y='Zimbabwe', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Havana', y='Cuba', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Helsinki', y='Finland', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Islamabad', y='Pakistan', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Jakarta', y='Indonesia', task='capital-world'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Kabul', y='Afghanistan', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Baku', y='Azerbaijan', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Bamako', y='Mali', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Bangkok', y='Thailand', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Banjul', y='Gambia', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Beijing', y='China', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Beirut', y='Lebanon', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Belgrade', y='Serbia', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Belmopan', y='Belize', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Berlin', y='Germany', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Bern', y='Switzerland', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Bishkek', y='Kyrgyzstan', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Bratislava', y='Slovakia', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Brussels', y='Belgium', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Bucharest', y='Romania', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Budapest', y='Hungary', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Bujumbura', y='Burundi', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Cairo', y='Egypt', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Canberra', y='Australia', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Caracas', y='Venezuela', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Chisinau', y='Moldova', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Conakry', y='Guinea', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Copenhagen', y='Denmark', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Dakar', y='Senegal', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Damascus', y='Syria', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Dhaka', y='Bangladesh', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Doha', y='Qatar', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Dublin', y='Ireland', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Dushanbe', y='Tajikistan', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Funafuti', y='Tuvalu', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Gaborone', y='Botswana', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Georgetown', y='Guyana', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Hanoi', y='Vietnam', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Harare', y='Zimbabwe', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Havana', y='Cuba', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Helsinki', y='Finland', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Islamabad', y='Pakistan', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Jakarta', y='Indonesia', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Kabul', y='Afghanistan', task='capital-world'),\n Raw_Datapoint(x1='Baghdad', x2='Iraq', x3='Kampala', y='Uganda', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Bamako', y='Mali', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Bangkok', y='Thailand', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Banjul', y='Gambia', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Beijing', y='China', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Beirut', y='Lebanon', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Belgrade', y='Serbia', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Belmopan', y='Belize', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Berlin', y='Germany', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Bern', y='Switzerland', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Bishkek', y='Kyrgyzstan', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Bratislava', y='Slovakia', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Brussels', y='Belgium', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Bucharest', y='Romania', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Budapest', y='Hungary', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Bujumbura', y='Burundi', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Cairo', y='Egypt', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Canberra', y='Australia', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Caracas', y='Venezuela', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Chisinau', y='Moldova', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Conakry', y='Guinea', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Copenhagen', y='Denmark', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Dakar', y='Senegal', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Damascus', y='Syria', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Dhaka', y='Bangladesh', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Doha', y='Qatar', task='capital-world'),\n Raw_Datapoint(x1='Baku', x2='Azerbaijan', x3='Dublin', y='Ireland', task='capital-world'),\n ...]"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the google analogies training dataset:\n",
    "analogy_dataset = load_data()\n",
    "analogy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data such that it includes the embeddings of the words in consideration\n",
    "transformed_analogy_dataset = transform_data(word_vectors, analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(300,)\n(100,)\n(1,)\n"
    }
   ],
   "source": [
    "# Testing the transformed analogy dataset\n",
    "print(transformed_analogy_dataset[0].analogy_embeddings.shape)\n",
    "print(transformed_analogy_dataset[0].gt_embedding.shape)\n",
    "print(transformed_analogy_dataset[0].protected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0/50] Running epoch\nepoch 0; iter: 0; batch classifier loss: 0.357857\n[1/50] Running epoch\nepoch 1; iter: 0; batch classifier loss: 0.346382\n[2/50] Running epoch\nepoch 2; iter: 0; batch classifier loss: 0.331825\n[3/50] Running epoch\nepoch 3; iter: 0; batch classifier loss: 0.354570\n[4/50] Running epoch\nepoch 4; iter: 0; batch classifier loss: 0.336471\n[5/50] Running epoch\nepoch 5; iter: 0; batch classifier loss: 0.341985\n[6/50] Running epoch\nepoch 6; iter: 0; batch classifier loss: 0.331365\n[7/50] Running epoch\nepoch 7; iter: 0; batch classifier loss: 0.325093\n[8/50] Running epoch\nepoch 8; iter: 0; batch classifier loss: 0.355655\n[9/50] Running epoch\nepoch 9; iter: 0; batch classifier loss: 0.323041\n[10/50] Running epoch\nepoch 10; iter: 0; batch classifier loss: 0.344972\n[11/50] Running epoch\nepoch 11; iter: 0; batch classifier loss: 0.309037\n[12/50] Running epoch\nepoch 12; iter: 0; batch classifier loss: 0.328624\n[13/50] Running epoch\nepoch 13; iter: 0; batch classifier loss: 0.344292\n[14/50] Running epoch\nepoch 14; iter: 0; batch classifier loss: 0.329110\n[15/50] Running epoch\nepoch 15; iter: 0; batch classifier loss: 0.349273\n[16/50] Running epoch\nepoch 16; iter: 0; batch classifier loss: 0.333126\n[17/50] Running epoch\nepoch 17; iter: 0; batch classifier loss: 0.331498\n[18/50] Running epoch\nepoch 18; iter: 0; batch classifier loss: 0.347967\n[19/50] Running epoch\nepoch 19; iter: 0; batch classifier loss: 0.311828\n[20/50] Running epoch\nepoch 20; iter: 0; batch classifier loss: 0.335061\n[21/50] Running epoch\nepoch 21; iter: 0; batch classifier loss: 0.318693\n[22/50] Running epoch\nepoch 22; iter: 0; batch classifier loss: 0.344584\n[23/50] Running epoch\nepoch 23; iter: 0; batch classifier loss: 0.342061\n[24/50] Running epoch\nepoch 24; iter: 0; batch classifier loss: 0.322517\n[25/50] Running epoch\nepoch 25; iter: 0; batch classifier loss: 0.330398\n[26/50] Running epoch\nepoch 26; iter: 0; batch classifier loss: 0.324075\n[27/50] Running epoch\nepoch 27; iter: 0; batch classifier loss: 0.328177\n[28/50] Running epoch\nepoch 28; iter: 0; batch classifier loss: 0.327361\n[29/50] Running epoch\nepoch 29; iter: 0; batch classifier loss: 0.336042\n[30/50] Running epoch\nepoch 30; iter: 0; batch classifier loss: 0.330819\n[31/50] Running epoch\nepoch 31; iter: 0; batch classifier loss: 0.345728\n[32/50] Running epoch\nepoch 32; iter: 0; batch classifier loss: 0.354661\n[33/50] Running epoch\nepoch 33; iter: 0; batch classifier loss: 0.344130\n[34/50] Running epoch\nepoch 34; iter: 0; batch classifier loss: 0.324162\n[35/50] Running epoch\nepoch 35; iter: 0; batch classifier loss: 0.323159\n[36/50] Running epoch\nepoch 36; iter: 0; batch classifier loss: 0.335285\n[37/50] Running epoch\nepoch 37; iter: 0; batch classifier loss: 0.301571\n[38/50] Running epoch\nepoch 38; iter: 0; batch classifier loss: 0.342569\n[39/50] Running epoch\nepoch 39; iter: 0; batch classifier loss: 0.344558\n[40/50] Running epoch\nepoch 40; iter: 0; batch classifier loss: 0.344592\n[41/50] Running epoch\nepoch 41; iter: 0; batch classifier loss: 0.314755\n[42/50] Running epoch\nepoch 42; iter: 0; batch classifier loss: 0.323937\n[43/50] Running epoch\nepoch 43; iter: 0; batch classifier loss: 0.323942\n[44/50] Running epoch\nepoch 44; iter: 0; batch classifier loss: 0.329149\n[45/50] Running epoch\nepoch 45; iter: 0; batch classifier loss: 0.341093\n[46/50] Running epoch\nepoch 46; iter: 0; batch classifier loss: 0.314521\n[47/50] Running epoch\nepoch 47; iter: 0; batch classifier loss: 0.335499\n[48/50] Running epoch\nepoch 48; iter: 0; batch classifier loss: 0.337509\n[49/50] Running epoch\nepoch 49; iter: 0; batch classifier loss: 0.322767\n"
    },
    {
     "data": {
      "text/plain": "<adversarial_debiasing.AdversarialDebiasing at 0x148ff0f59c8>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the variant of the model without debiasing\n",
    "non_debiased_model = AdversarialDebiasing(debias=False)\n",
    "non_debiased_model.fit(dataset=transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "poch 34; iter: 120; batch classifier loss: 0.335279; batch adversarial loss: 0.182830\nepoch 34; iter: 130; batch classifier loss: 0.349417; batch adversarial loss: 0.193133\nepoch 34; iter: 140; batch classifier loss: 0.350720; batch adversarial loss: 0.228916\n[35/50] Running epoch\nepoch 35; iter: 0; batch classifier loss: 0.300946; batch adversarial loss: 0.202169\nepoch 35; iter: 10; batch classifier loss: 0.327983; batch adversarial loss: 0.159899\nepoch 35; iter: 20; batch classifier loss: 0.311907; batch adversarial loss: 0.196383\nepoch 35; iter: 30; batch classifier loss: 0.339024; batch adversarial loss: 0.171867\nepoch 35; iter: 40; batch classifier loss: 0.348780; batch adversarial loss: 0.236129\nepoch 35; iter: 50; batch classifier loss: 0.343712; batch adversarial loss: 0.222972\nepoch 35; iter: 60; batch classifier loss: 0.357575; batch adversarial loss: 0.185604\nepoch 35; iter: 70; batch classifier loss: 0.365883; batch adversarial loss: 0.227271\nepoch 35; iter: 80; batch classifier loss: 0.357714; batch adversarial loss: 0.199453\nepoch 35; iter: 90; batch classifier loss: 0.333407; batch adversarial loss: 0.192413\nepoch 35; iter: 100; batch classifier loss: 0.351624; batch adversarial loss: 0.184197\nepoch 35; iter: 110; batch classifier loss: 0.329982; batch adversarial loss: 0.151283\nepoch 35; iter: 120; batch classifier loss: 0.324125; batch adversarial loss: 0.202246\nepoch 35; iter: 130; batch classifier loss: 0.366605; batch adversarial loss: 0.256403\nepoch 35; iter: 140; batch classifier loss: 0.332518; batch adversarial loss: 0.256851\n[36/50] Running epoch\nepoch 36; iter: 0; batch classifier loss: 0.328165; batch adversarial loss: 0.209794\nepoch 36; iter: 10; batch classifier loss: 0.344315; batch adversarial loss: 0.230566\nepoch 36; iter: 20; batch classifier loss: 0.347345; batch adversarial loss: 0.181160\nepoch 36; iter: 30; batch classifier loss: 0.341644; batch adversarial loss: 0.193772\nepoch 36; iter: 40; batch classifier loss: 0.329963; batch adversarial loss: 0.240484\nepoch 36; iter: 50; batch classifier loss: 0.327013; batch adversarial loss: 0.209325\nepoch 36; iter: 60; batch classifier loss: 0.328388; batch adversarial loss: 0.149442\nepoch 36; iter: 70; batch classifier loss: 0.338836; batch adversarial loss: 0.171599\nepoch 36; iter: 80; batch classifier loss: 0.322679; batch adversarial loss: 0.222596\nepoch 36; iter: 90; batch classifier loss: 0.342924; batch adversarial loss: 0.167419\nepoch 36; iter: 100; batch classifier loss: 0.361792; batch adversarial loss: 0.220393\nepoch 36; iter: 110; batch classifier loss: 0.358886; batch adversarial loss: 0.260539\nepoch 36; iter: 120; batch classifier loss: 0.343846; batch adversarial loss: 0.263467\nepoch 36; iter: 130; batch classifier loss: 0.325837; batch adversarial loss: 0.166045\nepoch 36; iter: 140; batch classifier loss: 0.343754; batch adversarial loss: 0.205175\n[37/50] Running epoch\nepoch 37; iter: 0; batch classifier loss: 0.346233; batch adversarial loss: 0.205706\nepoch 37; iter: 10; batch classifier loss: 0.337830; batch adversarial loss: 0.251404\nepoch 37; iter: 20; batch classifier loss: 0.332650; batch adversarial loss: 0.178161\nepoch 37; iter: 30; batch classifier loss: 0.337745; batch adversarial loss: 0.217548\nepoch 37; iter: 40; batch classifier loss: 0.356412; batch adversarial loss: 0.222740\nepoch 37; iter: 50; batch classifier loss: 0.347919; batch adversarial loss: 0.200482\nepoch 37; iter: 60; batch classifier loss: 0.349359; batch adversarial loss: 0.243193\nepoch 37; iter: 70; batch classifier loss: 0.327907; batch adversarial loss: 0.227416\nepoch 37; iter: 80; batch classifier loss: 0.343331; batch adversarial loss: 0.220514\nepoch 37; iter: 90; batch classifier loss: 0.314387; batch adversarial loss: 0.183492\nepoch 37; iter: 100; batch classifier loss: 0.327548; batch adversarial loss: 0.194210\nepoch 37; iter: 110; batch classifier loss: 0.324895; batch adversarial loss: 0.165664\nepoch 37; iter: 120; batch classifier loss: 0.370270; batch adversarial loss: 0.241869\nepoch 37; iter: 130; batch classifier loss: 0.364555; batch adversarial loss: 0.193878\nepoch 37; iter: 140; batch classifier loss: 0.327981; batch adversarial loss: 0.177813\n[38/50] Running epoch\nepoch 38; iter: 0; batch classifier loss: 0.328789; batch adversarial loss: 0.225292\nepoch 38; iter: 10; batch classifier loss: 0.333866; batch adversarial loss: 0.228782\nepoch 38; iter: 20; batch classifier loss: 0.323003; batch adversarial loss: 0.199524\nepoch 38; iter: 30; batch classifier loss: 0.326169; batch adversarial loss: 0.225570\nepoch 38; iter: 40; batch classifier loss: 0.347350; batch adversarial loss: 0.206045\nepoch 38; iter: 50; batch classifier loss: 0.336395; batch adversarial loss: 0.189260\nepoch 38; iter: 60; batch classifier loss: 0.343864; batch adversarial loss: 0.210413\nepoch 38; iter: 70; batch classifier loss: 0.338449; batch adversarial loss: 0.206692\nepoch 38; iter: 80; batch classifier loss: 0.353872; batch adversarial loss: 0.157115\nepoch 38; iter: 90; batch classifier loss: 0.338389; batch adversarial loss: 0.227553\nepoch 38; iter: 100; batch classifier loss: 0.333653; batch adversarial loss: 0.155965\nepoch 38; iter: 110; batch classifier loss: 0.310249; batch adversarial loss: 0.220040\nepoch 38; iter: 120; batch classifier loss: 0.321990; batch adversarial loss: 0.272570\nepoch 38; iter: 130; batch classifier loss: 0.340458; batch adversarial loss: 0.234645\nepoch 38; iter: 140; batch classifier loss: 0.336718; batch adversarial loss: 0.230125\n[39/50] Running epoch\nepoch 39; iter: 0; batch classifier loss: 0.361808; batch adversarial loss: 0.188107\nepoch 39; iter: 10; batch classifier loss: 0.336764; batch adversarial loss: 0.229629\nepoch 39; iter: 20; batch classifier loss: 0.346521; batch adversarial loss: 0.203279\nepoch 39; iter: 30; batch classifier loss: 0.331288; batch adversarial loss: 0.188921\nepoch 39; iter: 40; batch classifier loss: 0.341941; batch adversarial loss: 0.187146\nepoch 39; iter: 50; batch classifier loss: 0.306042; batch adversarial loss: 0.165699\nepoch 39; iter: 60; batch classifier loss: 0.336027; batch adversarial loss: 0.217979\nepoch 39; iter: 70; batch classifier loss: 0.336685; batch adversarial loss: 0.254966\nepoch 39; iter: 80; batch classifier loss: 0.325395; batch adversarial loss: 0.163442\nepoch 39; iter: 90; batch classifier loss: 0.355078; batch adversarial loss: 0.263622\nepoch 39; iter: 100; batch classifier loss: 0.334605; batch adversarial loss: 0.236406\nepoch 39; iter: 110; batch classifier loss: 0.344402; batch adversarial loss: 0.231633\nepoch 39; iter: 120; batch classifier loss: 0.319045; batch adversarial loss: 0.166911\nepoch 39; iter: 130; batch classifier loss: 0.313342; batch adversarial loss: 0.186792\nepoch 39; iter: 140; batch classifier loss: 0.335979; batch adversarial loss: 0.204755\n[40/50] Running epoch\nepoch 40; iter: 0; batch classifier loss: 0.340826; batch adversarial loss: 0.206760\nepoch 40; iter: 10; batch classifier loss: 0.326049; batch adversarial loss: 0.210182\nepoch 40; iter: 20; batch classifier loss: 0.330633; batch adversarial loss: 0.287950\nepoch 40; iter: 30; batch classifier loss: 0.338170; batch adversarial loss: 0.202881\nepoch 40; iter: 40; batch classifier loss: 0.349860; batch adversarial loss: 0.189707\nepoch 40; iter: 50; batch classifier loss: 0.345035; batch adversarial loss: 0.252822\nepoch 40; iter: 60; batch classifier loss: 0.336400; batch adversarial loss: 0.270821\nepoch 40; iter: 70; batch classifier loss: 0.355861; batch adversarial loss: 0.195326\nepoch 40; iter: 80; batch classifier loss: 0.344572; batch adversarial loss: 0.312752\nepoch 40; iter: 90; batch classifier loss: 0.337193; batch adversarial loss: 0.214888\nepoch 40; iter: 100; batch classifier loss: 0.346496; batch adversarial loss: 0.192108\nepoch 40; iter: 110; batch classifier loss: 0.323241; batch adversarial loss: 0.186770\nepoch 40; iter: 120; batch classifier loss: 0.341405; batch adversarial loss: 0.228806\nepoch 40; iter: 130; batch classifier loss: 0.334342; batch adversarial loss: 0.198482\nepoch 40; iter: 140; batch classifier loss: 0.335063; batch adversarial loss: 0.225042\n[41/50] Running epoch\nepoch 41; iter: 0; batch classifier loss: 0.339456; batch adversarial loss: 0.227861\nepoch 41; iter: 10; batch classifier loss: 0.352139; batch adversarial loss: 0.178874\nepoch 41; iter: 20; batch classifier loss: 0.344538; batch adversarial loss: 0.258321\nepoch 41; iter: 30; batch classifier loss: 0.345336; batch adversarial loss: 0.165381\nepoch 41; iter: 40; batch classifier loss: 0.343065; batch adversarial loss: 0.205459\nepoch 41; iter: 50; batch classifier loss: 0.349256; batch adversarial loss: 0.247707\nepoch 41; iter: 60; batch classifier loss: 0.342530; batch adversarial loss: 0.209903\nepoch 41; iter: 70; batch classifier loss: 0.329325; batch adversarial loss: 0.222014\nepoch 41; iter: 80; batch classifier loss: 0.334587; batch adversarial loss: 0.239782\nepoch 41; iter: 90; batch classifier loss: 0.327114; batch adversarial loss: 0.218612\nepoch 41; iter: 100; batch classifier loss: 0.325643; batch adversarial loss: 0.254430\nepoch 41; iter: 110; batch classifier loss: 0.330258; batch adversarial loss: 0.245296\nepoch 41; iter: 120; batch classifier loss: 0.313150; batch adversarial loss: 0.183939\nepoch 41; iter: 130; batch classifier loss: 0.331795; batch adversarial loss: 0.252555\nepoch 41; iter: 140; batch classifier loss: 0.344242; batch adversarial loss: 0.223166\n[42/50] Running epoch\nepoch 42; iter: 0; batch classifier loss: 0.352535; batch adversarial loss: 0.223426\nepoch 42; iter: 10; batch classifier loss: 0.346746; batch adversarial loss: 0.212484\nepoch 42; iter: 20; batch classifier loss: 0.332096; batch adversarial loss: 0.290136\nepoch 42; iter: 30; batch classifier loss: 0.313745; batch adversarial loss: 0.214775\nepoch 42; iter: 40; batch classifier loss: 0.339515; batch adversarial loss: 0.259908\nepoch 42; iter: 50; batch classifier loss: 0.321052; batch adversarial loss: 0.224400\nepoch 42; iter: 60; batch classifier loss: 0.361222; batch adversarial loss: 0.239685\nepoch 42; iter: 70; batch classifier loss: 0.325962; batch adversarial loss: 0.188537\nepoch 42; iter: 80; batch classifier loss: 0.334147; batch adversarial loss: 0.258963\nepoch 42; iter: 90; batch classifier loss: 0.346544; batch adversarial loss: 0.213929\nepoch 42; iter: 100; batch classifier loss: 0.337701; batch adversarial loss: 0.222091\nepoch 42; iter: 110; batch classifier loss: 0.349527; batch adversarial loss: 0.235030\nepoch 42; iter: 120; batch classifier loss: 0.366377; batch adversarial loss: 0.192819\nepoch 42; iter: 130; batch classifier loss: 0.329953; batch adversarial loss: 0.184771\nepoch 42; iter: 140; batch classifier loss: 0.339867; batch adversarial loss: 0.222113\n[43/50] Running epoch\nepoch 43; iter: 0; batch classifier loss: 0.334470; batch adversarial loss: 0.230515\nepoch 43; iter: 10; batch classifier loss: 0.338608; batch adversarial loss: 0.184650\nepoch 43; iter: 20; batch classifier loss: 0.348237; batch adversarial loss: 0.199253\nepoch 43; iter: 30; batch classifier loss: 0.321348; batch adversarial loss: 0.220674\nepoch 43; iter: 40; batch classifier loss: 0.344009; batch adversarial loss: 0.210095\nepoch 43; iter: 50; batch classifier loss: 0.370934; batch adversarial loss: 0.240156\nepoch 43; iter: 60; batch classifier loss: 0.324629; batch adversarial loss: 0.212634\nepoch 43; iter: 70; batch classifier loss: 0.339998; batch adversarial loss: 0.184922\nepoch 43; iter: 80; batch classifier loss: 0.331007; batch adversarial loss: 0.228504\nepoch 43; iter: 90; batch classifier loss: 0.337926; batch adversarial loss: 0.237715\nepoch 43; iter: 100; batch classifier loss: 0.333336; batch adversarial loss: 0.147803\nepoch 43; iter: 110; batch classifier loss: 0.337721; batch adversarial loss: 0.206656\nepoch 43; iter: 120; batch classifier loss: 0.336505; batch adversarial loss: 0.174578\nepoch 43; iter: 130; batch classifier loss: 0.332508; batch adversarial loss: 0.191809\nepoch 43; iter: 140; batch classifier loss: 0.329956; batch adversarial loss: 0.204409\n[44/50] Running epoch\nepoch 44; iter: 0; batch classifier loss: 0.343577; batch adversarial loss: 0.225188\nepoch 44; iter: 10; batch classifier loss: 0.336520; batch adversarial loss: 0.271331\nepoch 44; iter: 20; batch classifier loss: 0.311955; batch adversarial loss: 0.200855\nepoch 44; iter: 30; batch classifier loss: 0.340992; batch adversarial loss: 0.191263\nepoch 44; iter: 40; batch classifier loss: 0.350625; batch adversarial loss: 0.205910\nepoch 44; iter: 50; batch classifier loss: 0.349100; batch adversarial loss: 0.185912\nepoch 44; iter: 60; batch classifier loss: 0.323120; batch adversarial loss: 0.240058\nepoch 44; iter: 70; batch classifier loss: 0.337414; batch adversarial loss: 0.240294\nepoch 44; iter: 80; batch classifier loss: 0.321787; batch adversarial loss: 0.217891\nepoch 44; iter: 90; batch classifier loss: 0.341076; batch adversarial loss: 0.171027\nepoch 44; iter: 100; batch classifier loss: 0.350028; batch adversarial loss: 0.274578\nepoch 44; iter: 110; batch classifier loss: 0.332825; batch adversarial loss: 0.269999\nepoch 44; iter: 120; batch classifier loss: 0.340819; batch adversarial loss: 0.295824\nepoch 44; iter: 130; batch classifier loss: 0.325170; batch adversarial loss: 0.198573\nepoch 44; iter: 140; batch classifier loss: 0.330634; batch adversarial loss: 0.188266\n[45/50] Running epoch\nepoch 45; iter: 0; batch classifier loss: 0.350783; batch adversarial loss: 0.202469\nepoch 45; iter: 10; batch classifier loss: 0.339981; batch adversarial loss: 0.187716\nepoch 45; iter: 20; batch classifier loss: 0.348874; batch adversarial loss: 0.188417\nepoch 45; iter: 30; batch classifier loss: 0.328213; batch adversarial loss: 0.208753\nepoch 45; iter: 40; batch classifier loss: 0.330618; batch adversarial loss: 0.195000\nepoch 45; iter: 50; batch classifier loss: 0.349806; batch adversarial loss: 0.267904\nepoch 45; iter: 60; batch classifier loss: 0.336835; batch adversarial loss: 0.215696\nepoch 45; iter: 70; batch classifier loss: 0.330786; batch adversarial loss: 0.192295\nepoch 45; iter: 80; batch classifier loss: 0.356562; batch adversarial loss: 0.231205\nepoch 45; iter: 90; batch classifier loss: 0.326852; batch adversarial loss: 0.228180\nepoch 45; iter: 100; batch classifier loss: 0.350758; batch adversarial loss: 0.192342\nepoch 45; iter: 110; batch classifier loss: 0.317698; batch adversarial loss: 0.172422\nepoch 45; iter: 120; batch classifier loss: 0.332852; batch adversarial loss: 0.200832\nepoch 45; iter: 130; batch classifier loss: 0.335986; batch adversarial loss: 0.192661\nepoch 45; iter: 140; batch classifier loss: 0.349173; batch adversarial loss: 0.183034\n[46/50] Running epoch\nepoch 46; iter: 0; batch classifier loss: 0.330443; batch adversarial loss: 0.202200\nepoch 46; iter: 10; batch classifier loss: 0.347321; batch adversarial loss: 0.208172\nepoch 46; iter: 20; batch classifier loss: 0.344495; batch adversarial loss: 0.215909\nepoch 46; iter: 30; batch classifier loss: 0.325541; batch adversarial loss: 0.177771\nepoch 46; iter: 40; batch classifier loss: 0.361882; batch adversarial loss: 0.169820\nepoch 46; iter: 50; batch classifier loss: 0.335911; batch adversarial loss: 0.173859\nepoch 46; iter: 60; batch classifier loss: 0.330352; batch adversarial loss: 0.225626\nepoch 46; iter: 70; batch classifier loss: 0.349397; batch adversarial loss: 0.234353\nepoch 46; iter: 80; batch classifier loss: 0.344396; batch adversarial loss: 0.227633\nepoch 46; iter: 90; batch classifier loss: 0.354445; batch adversarial loss: 0.311543\nepoch 46; iter: 100; batch classifier loss: 0.333460; batch adversarial loss: 0.226062\nepoch 46; iter: 110; batch classifier loss: 0.325119; batch adversarial loss: 0.169849\nepoch 46; iter: 120; batch classifier loss: 0.327891; batch adversarial loss: 0.249098\nepoch 46; iter: 130; batch classifier loss: 0.341069; batch adversarial loss: 0.190671\nepoch 46; iter: 140; batch classifier loss: 0.340812; batch adversarial loss: 0.203065\n[47/50] Running epoch\nepoch 47; iter: 0; batch classifier loss: 0.335282; batch adversarial loss: 0.182207\nepoch 47; iter: 10; batch classifier loss: 0.350060; batch adversarial loss: 0.225616\nepoch 47; iter: 20; batch classifier loss: 0.316026; batch adversarial loss: 0.169048\nepoch 47; iter: 30; batch classifier loss: 0.331588; batch adversarial loss: 0.154924\nepoch 47; iter: 40; batch classifier loss: 0.336849; batch adversarial loss: 0.189371\nepoch 47; iter: 50; batch classifier loss: 0.356259; batch adversarial loss: 0.183171\nepoch 47; iter: 60; batch classifier loss: 0.325189; batch adversarial loss: 0.205622\nepoch 47; iter: 70; batch classifier loss: 0.325741; batch adversarial loss: 0.185396\nepoch 47; iter: 80; batch classifier loss: 0.344229; batch adversarial loss: 0.233708\nepoch 47; iter: 90; batch classifier loss: 0.313273; batch adversarial loss: 0.203227\nepoch 47; iter: 100; batch classifier loss: 0.309975; batch adversarial loss: 0.166945\nepoch 47; iter: 110; batch classifier loss: 0.340223; batch adversarial loss: 0.181837\nepoch 47; iter: 120; batch classifier loss: 0.320362; batch adversarial loss: 0.194616\nepoch 47; iter: 130; batch classifier loss: 0.368065; batch adversarial loss: 0.197229\nepoch 47; iter: 140; batch classifier loss: 0.339030; batch adversarial loss: 0.223357\n[48/50] Running epoch\nepoch 48; iter: 0; batch classifier loss: 0.320439; batch adversarial loss: 0.231992\nepoch 48; iter: 10; batch classifier loss: 0.332124; batch adversarial loss: 0.195250\nepoch 48; iter: 20; batch classifier loss: 0.333385; batch adversarial loss: 0.189784\nepoch 48; iter: 30; batch classifier loss: 0.345053; batch adversarial loss: 0.190366\nepoch 48; iter: 40; batch classifier loss: 0.321330; batch adversarial loss: 0.211673\nepoch 48; iter: 50; batch classifier loss: 0.352286; batch adversarial loss: 0.229981\nepoch 48; iter: 60; batch classifier loss: 0.327871; batch adversarial loss: 0.181559\nepoch 48; iter: 70; batch classifier loss: 0.345455; batch adversarial loss: 0.259748\nepoch 48; iter: 80; batch classifier loss: 0.328536; batch adversarial loss: 0.185387\nepoch 48; iter: 90; batch classifier loss: 0.361338; batch adversarial loss: 0.174462\nepoch 48; iter: 100; batch classifier loss: 0.326101; batch adversarial loss: 0.208363\nepoch 48; iter: 110; batch classifier loss: 0.328732; batch adversarial loss: 0.204905\nepoch 48; iter: 120; batch classifier loss: 0.327362; batch adversarial loss: 0.205867\nepoch 48; iter: 130; batch classifier loss: 0.340400; batch adversarial loss: 0.169958\nepoch 48; iter: 140; batch classifier loss: 0.313020; batch adversarial loss: 0.215334\n[49/50] Running epoch\nepoch 49; iter: 0; batch classifier loss: 0.319907; batch adversarial loss: 0.197200\nepoch 49; iter: 10; batch classifier loss: 0.336005; batch adversarial loss: 0.228833\nepoch 49; iter: 20; batch classifier loss: 0.321304; batch adversarial loss: 0.218069\nepoch 49; iter: 30; batch classifier loss: 0.333425; batch adversarial loss: 0.188797\nepoch 49; iter: 40; batch classifier loss: 0.347039; batch adversarial loss: 0.193851\nepoch 49; iter: 50; batch classifier loss: 0.355999; batch adversarial loss: 0.187727\nepoch 49; iter: 60; batch classifier loss: 0.354037; batch adversarial loss: 0.193815\nepoch 49; iter: 70; batch classifier loss: 0.337666; batch adversarial loss: 0.203458\nepoch 49; iter: 80; batch classifier loss: 0.331481; batch adversarial loss: 0.314618\nepoch 49; iter: 90; batch classifier loss: 0.330134; batch adversarial loss: 0.238472\nepoch 49; iter: 100; batch classifier loss: 0.341618; batch adversarial loss: 0.180441\nepoch 49; iter: 110; batch classifier loss: 0.335037; batch adversarial loss: 0.208356\nepoch 49; iter: 120; batch classifier loss: 0.331702; batch adversarial loss: 0.218304\nepoch 49; iter: 130; batch classifier loss: 0.331409; batch adversarial loss: 0.189704\nepoch 49; iter: 140; batch classifier loss: 0.326981; batch adversarial loss: 0.204257\n"
    },
    {
     "data": {
      "text/plain": "<adversarial_debiasing.AdversarialDebiasing at 0x148ffbd62c8>"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the variant of the model with debiasing\n",
    "debiased_model = AdversarialDebiasing()\n",
    "debiased_model.fit(dataset=transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(20, 300)\n"
    }
   ],
   "source": [
    "# Examples to test the models upon\n",
    "datapoints, test_analogies = [], []\n",
    "with open(os.path.join('data', 'sexism-traps.txt'), 'r') as f:\n",
    "    # Reading each line\n",
    "    for line in f.readlines():\n",
    "        words = line.split()\n",
    "        if words[0] == ':':\n",
    "            continue\n",
    "        test_analogies.append(words)\n",
    "        word_embeddings = word_vectors[words]\n",
    "        word_embeddings = np.reshape(word_embeddings, (1, -1))\n",
    "        datapoints.append(word_embeddings)\n",
    "datapoints = np.vstack(datapoints)\n",
    "print(datapoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation of the non-debiased model\n",
    "non_debiased_predictions = non_debiased_model.predict(datapoints)\n",
    "non_debiased_most_similar_list = utility_functions.obtain_most_similar(non_debiased_predictions, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the similarity list for the non-debiased model\n",
    "non_debiased_most_similar_list_data_frames = []\n",
    "for i in range(len(non_debiased_most_similar_list)):\n",
    "    # print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    temp_data_frame = pd.DataFrame(non_debiased_most_similar_list[i][1:], columns = ['Neighbor', 'Similarity'])\n",
    "    non_debiased_most_similar_list_data_frames.append(temp_data_frame)\n",
    "    # print(tabulate(temp_data_frame, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation of the debiased model\n",
    "debiased_predictions = debiased_model.predict(datapoints)\n",
    "debiased_most_similar_list = utility_functions.obtain_most_similar(debiased_predictions, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the similarity list for the debiased model\n",
    "debiased_most_similar_list_data_frames = []\n",
    "for i in range(len(debiased_most_similar_list)):\n",
    "    # print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    temp_data_frame = pd.DataFrame(debiased_most_similar_list[i][1:], columns = ['Neighbor', 'Similarity'])\n",
    "    debiased_most_similar_list_data_frames.append(temp_data_frame)\n",
    "    # print(tabulate(temp_data_frame, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "he : strong :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| strong      |        0.826 | strong      |        0.785 |\n| heart       |        0.706 | heart       |        0.728 |\n| stay        |        0.695 | always      |        0.684 |\n| keep        |        0.679 | girl        |        0.681 |\n| calm        |        0.678 | keep        |        0.661 |\n| girl        |        0.668 | she         |        0.651 |\n| always      |        0.666 | your        |        0.649 |\n| loving      |        0.648 | stay        |        0.645 |\n| 're         |        0.644 | really      |        0.644 |\n+-------------+--------------+-------------+--------------+\nhe : boss :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| boss        |        0.761 | boss        |        0.74  |\n| girl        |        0.671 | momma       |        0.696 |\n| she         |        0.657 | girl        |        0.68  |\n| momma       |        0.651 | she         |        0.674 |\n| tell        |        0.636 | mom         |        0.665 |\n| mommy       |        0.628 | mommy       |        0.662 |\n| wife        |        0.626 | gurl        |        0.655 |\n| gurl        |        0.625 | swear       |        0.644 |\n| swear       |        0.623 | tell        |        0.642 |\n+-------------+--------------+-------------+--------------+\nhe : company :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| company     |        0.806 | company     |        0.795 |\n| business    |        0.727 | business    |        0.714 |\n| group       |        0.655 | works       |        0.647 |\n| industry    |        0.648 | brand       |        0.644 |\n| companies   |        0.645 | companies   |        0.64  |\n| brand       |        0.641 | industry    |        0.636 |\n| partners    |        0.634 | find        |        0.635 |\n| works       |        0.633 | product     |        0.632 |\n| private     |        0.626 | shop        |        0.629 |\n+-------------+--------------+-------------+--------------+\nhe : athletic :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| athletic    |        0.765 | athletic    |        0.755 |\n| girls       |        0.654 | girls       |        0.649 |\n| women       |        0.615 | women       |        0.632 |\n| soccer      |        0.596 | female      |        0.575 |\n| ladies      |        0.579 | soccer      |        0.572 |\n| skinny      |        0.574 | classy      |        0.572 |\n| volleyball  |        0.563 | ladies      |        0.571 |\n| classy      |        0.559 | skinny      |        0.569 |\n| female      |        0.557 | models      |        0.567 |\n+-------------+--------------+-------------+--------------+\nhe : doctor :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| doctor      |        0.733 | doctor      |        0.777 |\n| mom         |        0.664 | mom         |        0.686 |\n| girl        |        0.662 | mother      |        0.674 |\n| sister      |        0.661 | sister      |        0.666 |\n| mommy       |        0.652 | aunt        |        0.664 |\n| mother      |        0.65  | girl        |        0.659 |\n| she         |        0.636 | mommy       |        0.658 |\n| aunt        |        0.634 | grandma     |        0.653 |\n| book        |        0.633 | moms        |        0.646 |\n+-------------+--------------+-------------+--------------+\nhe : leader :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| leader      |        0.788 | leader      |        0.767 |\n| member      |        0.654 | member      |        0.647 |\n| group       |        0.63  | members     |        0.603 |\n| members     |        0.618 | girl        |        0.595 |\n| girl        |        0.583 | group       |        0.591 |\n| dancer      |        0.571 | president   |        0.576 |\n| female      |        0.569 | dancer      |        0.575 |\n| chief       |        0.557 | who         |        0.575 |\n| president   |        0.552 | generation  |        0.574 |\n+-------------+--------------+-------------+--------------+\nhe : director :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| director    |        0.731 | director    |        0.747 |\n| editor      |        0.632 | assistant   |        0.622 |\n| assistant   |        0.62  | stylist     |        0.619 |\n| producer    |        0.611 | producer    |        0.608 |\n| academy     |        0.61  | writer      |        0.604 |\n| executive   |        0.61  | editor      |        0.603 |\n| writer      |        0.596 | vp          |        0.599 |\n| group       |        0.596 | exec        |        0.597 |\n| company     |        0.592 | executive   |        0.595 |\n+-------------+--------------+-------------+--------------+\nhe : rich :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| rich        |        0.832 | rich        |        0.843 |\n| fat         |        0.672 | fat         |        0.678 |\n| girl        |        0.653 | girl        |        0.659 |\n| cuz         |        0.643 | cuz         |        0.647 |\n| ratchet     |        0.639 | momma       |        0.645 |\n| swear       |        0.638 | swear       |        0.643 |\n| ghetto      |        0.631 | ratchet     |        0.643 |\n| virgin      |        0.63  | chick       |        0.641 |\n| chick       |        0.626 | ghetto      |        0.641 |\n+-------------+--------------+-------------+--------------+\nhe : pilot :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| pilot       |        0.767 | pilot       |        0.696 |\n| virgin      |        0.592 | model       |        0.542 |\n| rider       |        0.566 | mom         |        0.53  |\n| jet         |        0.552 | daughter    |        0.529 |\n| model       |        0.53  | nanny       |        0.516 |\n| plane       |        0.528 | tank        |        0.515 |\n| diary       |        0.521 | moms        |        0.514 |\n| princess    |        0.51  | pregnant    |        0.512 |\n| flight      |        0.508 | slave       |        0.507 |\n+-------------+--------------+-------------+--------------+\nhe : captain :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| captain     |        0.767 | captain     |        0.706 |\n| princess    |        0.613 | name        |        0.615 |\n| named       |        0.595 | princess    |        0.607 |\n| jasmine     |        0.593 | daughter    |        0.603 |\n| girl        |        0.587 | girl        |        0.599 |\n| tori        |        0.58  | named       |        0.595 |\n| brittany    |        0.577 | gonna       |        0.589 |\n| called      |        0.574 | think       |        0.582 |\n| name        |        0.57  | called      |        0.582 |\n+-------------+--------------+-------------+--------------+\nhe : president :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| president   |        0.799 | president   |        0.811 |\n| says        |        0.681 | says        |        0.676 |\n| barack      |        0.628 | barack      |        0.664 |\n| chief       |        0.626 | clinton     |        0.643 |\n| minister    |        0.621 | michelle    |        0.643 |\n| tells       |        0.619 | prez        |        0.642 |\n| s          |        0.618 | s          |        0.641 |\n| michelle    |        0.611 | obama       |        0.639 |\n| clinton     |        0.61  | tells       |        0.637 |\n+-------------+--------------+-------------+--------------+\nhe : power :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| power       |        0.771 | power       |        0.772 |\n| your        |        0.657 | your        |        0.674 |\n| and         |        0.656 | need        |        0.663 |\n| need        |        0.655 | my          |        0.655 |\n| now         |        0.654 | life        |        0.654 |\n| girl        |        0.641 | right       |        0.651 |\n| right       |        0.641 | and         |        0.651 |\n| life        |        0.64  | girl        |        0.649 |\n| turn        |        0.636 | heart       |        0.645 |\n+-------------+--------------+-------------+--------------+\nhe : rational :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| rational    |        0.77  | rational    |        0.757 |\n| logical     |        0.667 | logical     |        0.655 |\n| opinionated |        0.598 | reasoning   |        0.593 |\n| postive     |        0.57  | irrational  |        0.587 |\n| reasoning   |        0.562 | paradox     |        0.576 |\n| sympathetic |        0.561 | analytical  |        0.575 |\n| intuitive   |        0.557 | ethical     |        0.575 |\n| irrational  |        0.557 | simplistic  |        0.574 |\n| analytical  |        0.555 | indecisive  |        0.57  |\n+-------------+--------------+-------------+--------------+\nhe : confident :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| confident   |        0.8   | confident   |        0.737 |\n| fearless    |        0.627 | beyonce     |        0.615 |\n| pretty      |        0.613 | girl        |        0.608 |\n| girl        |        0.593 | pretty      |        0.604 |\n| shy         |        0.586 | obsessed    |        0.596 |\n| shes        |        0.586 | shes        |        0.594 |\n| calm        |        0.586 | youre       |        0.592 |\n| loving      |        0.585 | beyonc     |        0.592 |\n| insecure    |        0.584 | insecure    |        0.592 |\n+-------------+--------------+-------------+--------------+\nhe : hard :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| hard        |        0.822 | hard        |        0.827 |\n| girl        |        0.731 | girl        |        0.737 |\n| right       |        0.728 | right       |        0.731 |\n| swear       |        0.723 | swear       |        0.73  |\n| cuz         |        0.718 | gotta       |        0.729 |\n| cause       |        0.717 | really      |        0.728 |\n| really      |        0.715 | trying      |        0.726 |\n| gotta       |        0.714 | cuz         |        0.725 |\n| mad         |        0.713 | she         |        0.722 |\n+-------------+--------------+-------------+--------------+\nhe : relaxed :: she :\n+---------------+--------------+---------------+--------------+\n| Biased        |       Biased | Debiased      |     Debiased |\n| Neighbour     |   Similarity | Neighbour     |   Similarity |\n|---------------+--------------+---------------+--------------|\n| relaxed       |        0.788 | relaxed       |        0.753 |\n| comfortable   |        0.642 | comfortable   |        0.649 |\n| relaxing      |        0.596 | comfy         |        0.617 |\n| uncomfortable |        0.589 | messy         |        0.615 |\n| naturally     |        0.584 | uncomfortable |        0.614 |\n| comfy         |        0.581 | lazy          |        0.597 |\n| chilled       |        0.575 | feelin        |        0.584 |\n| productive    |        0.572 | relaxing      |        0.584 |\n| extremely     |        0.57  | naturally     |        0.579 |\n+---------------+--------------+---------------+--------------+\nhe : cry :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| cry         |        0.854 | cry         |        0.861 |\n| crying      |        0.745 | crying      |        0.747 |\n| scream      |        0.722 | scream      |        0.721 |\n| swear       |        0.707 | wanna       |        0.717 |\n| wanna       |        0.705 | swear       |        0.713 |\n| cant        |        0.701 | cant        |        0.711 |\n| girl        |        0.698 | girl        |        0.703 |\n| feel        |        0.685 | everytime   |        0.694 |\n| laugh       |        0.685 | laugh       |        0.692 |\n+-------------+--------------+-------------+--------------+\nhe : brave :: she :\n+-------------+--------------+-------------+--------------+\n| Biased      |       Biased | Debiased    |     Debiased |\n| Neighbour   |   Similarity | Neighbour   |   Similarity |\n|-------------+--------------+-------------+--------------|\n| brave       |        0.789 | brave       |        0.666 |\n| fearless    |        0.613 | girl        |        0.611 |\n| beautiful   |        0.61  | mother      |        0.601 |\n| loving      |        0.607 | mothers     |        0.59  |\n| girl        |        0.593 | daughter    |        0.585 |\n| sassy       |        0.589 | moms        |        0.582 |\n| gorgeous    |        0.575 | beautiful   |        0.582 |\n| pretty      |        0.572 | woman       |        0.581 |\n| woman       |        0.571 | hide        |        0.579 |\n+-------------+--------------+-------------+--------------+\nhe : intelligent :: she :\n+---------------+--------------+---------------+--------------+\n| Biased        |       Biased | Debiased      |     Debiased |\n| Neighbour     |   Similarity | Neighbour     |   Similarity |\n|---------------+--------------+---------------+--------------|\n| intelligent   |        0.802 | intelligent   |        0.734 |\n| mature        |        0.686 | person        |        0.613 |\n| extremely     |        0.633 | mature        |        0.607 |\n| educated      |        0.622 | extremely     |        0.604 |\n| tend          |        0.61  | tend          |        0.602 |\n| honest        |        0.605 | educated      |        0.6   |\n| naturally     |        0.596 | naturally     |        0.588 |\n| sophisticated |        0.595 | type          |        0.588 |\n| shy           |        0.594 | sophisticated |        0.584 |\n+---------------+--------------+---------------+--------------+\nhe : ambitious :: she :\n+---------------+--------------+---------------+--------------+\n| Biased        |       Biased | Debiased      |     Debiased |\n| Neighbour     |   Similarity | Neighbour     |   Similarity |\n|---------------+--------------+---------------+--------------|\n| ambitious     |        0.816 | ambitious     |        0.768 |\n| sophisticated |        0.596 | sophisticated |        0.59  |\n| fearless      |        0.588 | determined    |        0.562 |\n| feisty        |        0.565 | stubborn      |        0.561 |\n| conceited     |        0.564 | ambition      |        0.543 |\n| determined    |        0.562 | indecisive    |        0.536 |\n| stubborn      |        0.555 | hardworking   |        0.534 |\n| mature        |        0.544 | fearless      |        0.527 |\n| passionate    |        0.541 | materialistic |        0.524 |\n+---------------+--------------+---------------+--------------+\n"
    }
   ],
   "source": [
    "# Combining the dataframes pertaining to both the variants of the model\n",
    "iterables = [['Biased', 'Debiased'], ['Neighbour', 'Similarity']]\n",
    "index = pd.MultiIndex.from_product(iterables)\n",
    "overall_data_frames_list = []\n",
    "for i in range(len(non_debiased_most_similar_list)):\n",
    "    overall_list = []\n",
    "    print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    for j in range(len(non_debiased_most_similar_list[i][1:])):\n",
    "        temp_list = []\n",
    "        temp_list.append(non_debiased_most_similar_list[i][j][0])\n",
    "        temp_list.append(round(non_debiased_most_similar_list[i][j][1], 3))\n",
    "        temp_list.append(debiased_most_similar_list[i][j][0])\n",
    "        temp_list.append(round(debiased_most_similar_list[i][j][1], 3))\n",
    "        overall_list.append(temp_list)\n",
    "    temp_df = pd.DataFrame(overall_list, columns = index)\n",
    "    # print(temp_df.to_string(index = False))\n",
    "    print(tabulate(temp_df, headers = ['Biased\\nNeighbour', 'Biased\\nSimilarity', 'Debiased\\nNeighbour', 'Debiased\\nSimilarity'], tablefmt = 'psql', showindex = False))\n",
    "    overall_data_frames_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake dataset for testing purposes\n",
    "\n",
    "# embedding_dim = 100\n",
    "# analogy_dataset = [\n",
    "#     Datapoint(\n",
    "#     analogy_embeddings=np.random.normal(0, 1, size=(3 * embedding_dim, 1)), \n",
    "#     gt_embedding=np.random.normal(0, 1, size=(embedding_dim, 1)),\n",
    "#     protected_embedding=np.random.uniform(0, 1, size=(1))) for n in range(0, 1000)\n",
    "# ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}