{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from tabulate import tabulate\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from adversarial_debiasing import AdversarialDebiasing\n",
    "from load_data import load_data, transform_data, Datapoint\n",
    "\n",
    "from load_vectors import load_pretrained_vectors, load_vectors\n",
    "import config\n",
    "import utility_functions\n",
    "\n",
    "import gensim\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoreloading changes made in other python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the desired word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Wikipedia2Vec - use config.wiki_embedding_data_path and config.wiki_embedding_type\n",
    "# For Glove - use config.glove_embedding_data_path and config.glove_embedding_type\n",
    "# For GoogleNews (Word2Vec) - use config.google_embedding_data_path and config.google_embedding_type\n",
    "\n",
    "word_vectors = load_pretrained_vectors(config.google_embedding_data_path, config.save_dir, config.google_embedding_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Google Analogies Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_dataset = load_data()\n",
    "analogy_dataset[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the above dataset to include the respective word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_analogy_dataset, gender_subspace = transform_data(word_vectors, analogy_dataset, use_boluk = False)\n",
    "\n",
    "# Obtaining the dimensionality of the word embeddings\n",
    "word_embedding_dim = transformed_analogy_dataset[0].gt_embedding.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the dimensions of the transformed analogy dataset components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the transformed analogy dataset\n",
    "assert transformed_analogy_dataset[0].analogy_embeddings.shape[0] == word_embedding_dim * 3\n",
    "assert transformed_analogy_dataset[0].gt_embedding.shape[0] == word_embedding_dim\n",
    "assert transformed_analogy_dataset[0].protected.shape[0] == 1\n",
    "\n",
    "print(transformed_analogy_dataset[0].analogy_embeddings.shape)\n",
    "print(transformed_analogy_dataset[0].gt_embedding.shape)\n",
    "print(transformed_analogy_dataset[0].protected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To run the grid-search and obtain the np.dot(w.T, g) values\n",
    "# learning_rate_list = [2 ** -12, 2 ** -6, 2 ** -3]\n",
    "# adversary_loss_weight_list = [1.0, 0.5, 0.1]\n",
    "\n",
    "# # For the saved model checkpoints pertaining to the word embedding type\n",
    "# word_embedding_type = 'GNews'\n",
    "\n",
    "# # Performing the grid search\n",
    "# utility_functions.grid_search(learning_rate_list, adversary_loss_weight_list, word_embedding_dim, gender_subspace, transformed_analogy_dataset, word_embedding_type, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flag to indicate whether you want to use a pre-trained model or you want to train a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case you want to use a pre-trained model, then specify the type of word embeddings upon which the model was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_type = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case of using a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pretrained:\n",
    "    \n",
    "    # Obtaining the best weights for the non-debiased model\n",
    "    non_debiased_W1 = pretrained_parameters['non_debiased'][word_embedding_type][\"W1\"]\n",
    "    \n",
    "    # Creating an instance of the non-debiased model\n",
    "    non_debiased_model = AdversarialDebiasing(word_embedding_dim = word_embedding_dim, debias = False)\n",
    "    non_debiased_model.W1 = non_debiased_W1\n",
    "    \n",
    "    # Obtaining the best weights for the debiased model\n",
    "    debiased_W1 = pretrained_parameters['debiased'][word_embedding_type][\"W1\"]\n",
    "    \n",
    "    # Creating an instance of the debiased model\n",
    "    debiased_model = AdversarialDebiasing(word_embedding_dim = word_embedding_dim, debias = True)\n",
    "    debiased_model.W1 = debiased_W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case of using a model trained from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_pretrained:\n",
    "    \n",
    "    # Obtaining the best learning rate and adversary loss weight for the non-debiased model\n",
    "    non_debiased_learning_rate = pretrained_parameters['non_debiased'][word_embedding_type]['learning_rate']\n",
    "    non_debiased_adversary_loss_weight = pretrained_parameters['non_debiased'][word_embedding_type]['adversary_loss_weight']\n",
    "    \n",
    "    # Creating an instance of the non-debiased model\n",
    "    non_debiased_model = AdversarialDebiasing(word_embedding_dim = word_embedding_dim, num_epochs = 500, debias = False, \\\n",
    "                                             gender_subspace = gender_subspace, batch_size = 256, \\\n",
    "                                              adversary_loss_weight = non_debiased_adversary_loss_weight, \\\n",
    "                                             classifier_learning_rate = non_debiased_learning_rate, \\\n",
    "                                             adversary_learning_rate = non_debiased_learning_rate)\n",
    "    \n",
    "    # Fitting the non-debiased model to the training dataset\n",
    "    print(\"****************** Training the non-debiased model ********************\")\n",
    "    non_debiased_model.fit(dataset = transformed_analogy_dataset)\n",
    "    \n",
    "    # Obtaining the best learning rate and adversary loss weight for the non-debiased model\n",
    "    debiased_learning_rate = pretrained_parameters['debiased'][word_embedding_type]['learning_rate']\n",
    "    debiased_adversary_loss_weight = pretrained_parameters['debiased'][word_embedding_type]['adversary_loss_weight']\n",
    "    \n",
    "    # Creating an instance of the debiased model\n",
    "    debiased_model = AdversarialDebiasing(word_embedding_dim = word_embedding_dim, num_epochs = 500, debias = True, \\\n",
    "                                             gender_subspace = gender_subspace, batch_size = 256, \\\n",
    "                                              adversary_loss_weight = debiased_adversary_loss_weight, \\\n",
    "                                             classifier_learning_rate = debiased_learning_rate, \\\n",
    "                                             adversary_learning_rate = debiased_learning_rate)\n",
    "    \n",
    "    # Fitting the non-debiased model to the training dataset\n",
    "    print(\"****************** Training the debiased model ********************\")\n",
    "    debiased_model.fit(dataset = transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dim = transformed_analogy_dataset[0].gt_embedding.shape[0]\n",
    "# Training the variant of the model without debiasing\n",
    "non_debiased_model = AdversarialDebiasing(\n",
    "    word_embedding_dim=word_embedding_dim,\n",
    "    num_epochs=500,\n",
    "    debias=False,\n",
    "    gender_subspace=gender_subspace,\n",
    "    batch_size=256,\n",
    "    adversary_loss_weight=0.1,\n",
    "    classifier_learning_rate = 2 ** -6,\n",
    "    adversary_learning_rate = 2 ** -6\n",
    ")\n",
    "non_debiased_model.fit(dataset=transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = non_debiased_model.get_model_weights()\n",
    "print(np.dot(W1.detach().numpy().T,gender_subspace.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_subspace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the variant of the model with debiasing\n",
    "# debiased_model = AdversarialDebiasing(\n",
    "#     word_embedding_dim=word_embedding_dim,\n",
    "#     num_epochs=500,\n",
    "#     debias=True,\n",
    "#     gender_subspace=gender_subspace,\n",
    "#     batch_size=256,\n",
    "#     adversary_loss_weight=0.1,\n",
    "#     classifier_learning_rate = 2 ** -8,\n",
    "#     adversary_learning_rate = 2 ** -8\n",
    "# )\n",
    "debiased_model = AdversarialDebiasing(\n",
    "    word_embedding_dim=word_embedding_dim,\n",
    "    num_epochs=500,\n",
    "    debias=True,\n",
    "    gender_subspace=gender_subspace,\n",
    "    batch_size=256,\n",
    "    adversary_loss_weight=0.1,\n",
    "    classifier_learning_rate = 2 ** -6,\n",
    "    adversary_learning_rate = 2 ** -6\n",
    ")\n",
    "\n",
    "debiased_model.fit(dataset=transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = debiased_model.get_model_weights()\n",
    "print(np.dot(W1.clone().cpu().detach().numpy().T,gender_subspace.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples to test the models upon\n",
    "datapoints, test_analogies = [], []\n",
    "with open(os.path.join('data', 'sexism-traps.txt'), 'r') as f:\n",
    "    # Reading each line\n",
    "    for line in f.readlines():\n",
    "        words = line.split()\n",
    "        if words[0] == ':':\n",
    "            continue\n",
    "        test_analogies.append(words)\n",
    "        word_embeddings = word_vectors[words]\n",
    "        word_embeddings = np.reshape(word_embeddings, (1, -1))\n",
    "        datapoints.append(word_embeddings)\n",
    "datapoints = np.vstack(datapoints)\n",
    "print(datapoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation of the non-debiased model\n",
    "\n",
    "# non_debiased_predictions = non_debiased_model.predict(datapoints)\n",
    "features = torch.cat([torch.Tensor(x).unsqueeze_(0) for x in datapoints])\n",
    "x1 = features[:, 0:word_embedding_dim]\n",
    "x2 = features[:, word_embedding_dim:word_embedding_dim * 2]\n",
    "x3 = features[:, word_embedding_dim * 2:word_embedding_dim * 3]\n",
    "\n",
    "non_debiased_predictions = x2 + x3 - x1\n",
    "non_debiased_predictions = non_debiased_predictions.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_debiased_most_similar_list = utility_functions.obtain_most_similar(non_debiased_predictions, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the similarity list for the non-debiased model\n",
    "non_debiased_most_similar_list_data_frames = []\n",
    "for i in range(len(non_debiased_most_similar_list)):\n",
    "    # print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    temp_data_frame = pd.DataFrame(non_debiased_most_similar_list[i][1:], columns = ['Neighbor', 'Similarity'])\n",
    "    non_debiased_most_similar_list_data_frames.append(temp_data_frame)\n",
    "    # print(tabulate(temp_data_frame, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation of the debiased model\n",
    "debiased_predictions = debiased_model.predict(datapoints)\n",
    "debiased_most_similar_list = utility_functions.obtain_most_similar(debiased_predictions, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the similarity list for the debiased model\n",
    "debiased_most_similar_list_data_frames = []\n",
    "for i in range(len(debiased_most_similar_list)):\n",
    "    # print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    temp_data_frame = pd.DataFrame(debiased_most_similar_list[i][1:], columns = ['Neighbor', 'Similarity'])\n",
    "    debiased_most_similar_list_data_frames.append(temp_data_frame)\n",
    "    # print(tabulate(temp_data_frame, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the dataframes pertaining to both the variants of the model\n",
    "iterables = [['Biased', 'Debiased'], ['Neighbour', 'Similarity']]\n",
    "index = pd.MultiIndex.from_product(iterables)\n",
    "overall_data_frames_list = []\n",
    "for i in range(len(non_debiased_most_similar_list)):\n",
    "    overall_list = []\n",
    "    print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    for j in range(1, len(non_debiased_most_similar_list[i])):\n",
    "        temp_list = []\n",
    "        temp_list.append(non_debiased_most_similar_list[i][j][0])\n",
    "        temp_list.append(round(non_debiased_most_similar_list[i][j][1], 3))\n",
    "        temp_list.append(debiased_most_similar_list[i][j][0])\n",
    "        temp_list.append(round(debiased_most_similar_list[i][j][1], 3))\n",
    "        overall_list.append(temp_list)\n",
    "    temp_df = pd.DataFrame(overall_list, columns = index)\n",
    "    # print(temp_df.to_string(index = False))\n",
    "    print(tabulate(temp_df, headers = ['Biased\\nNeighbour', 'Biased\\nSimilarity', 'Debiased\\nNeighbour', 'Debiased\\nSimilarity'], tablefmt = 'psql', showindex = False))\n",
    "    overall_data_frames_list.append(temp_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondab37d419580d64adfa435fa6a50f93de5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
