{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from tabulate import tabulate\n",
    "import re \n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from adversarial_debiasing import AdversarialDebiasing\n",
    "from load_data import load_data, transform_data, Datapoint\n",
    "\n",
    "from load_vectors import load_pretrained_vectors, load_vectors\n",
    "import config\n",
    "import utility_functions\n",
    "\n",
    "import gensim\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# For autoreloading changes made in other python scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-08653ffbf64b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# For Glove - use config.glove_embedding_data_path and config.glove_embedding_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# For GoogleNews (Word2Vec) - use config.google_embedding_data_path and config.google_embedding_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_embedding_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_embedding_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/develop/master_develop/FACT-AI/load_vectors.py\u001b[0m in \u001b[0;36mload_pretrained_vectors\u001b[0;34m(data_path, savedir, embedding_type)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'google'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutility_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/GoogleNews-vectors-negative300.bin.gz'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error"
    }
   ],
   "source": [
    "# Loading the word vectors dictionary\n",
    "# For Wikipedia2Vec - use config.wiki_embedding_data_path and config.wiki_embedding_type\n",
    "# For Glove - use config.glove_embedding_data_path and config.glove_embedding_type\n",
    "# For GoogleNews (Word2Vec) - use config.google_embedding_data_path and config.google_embedding_type\n",
    "word_vectors = load_pretrained_vectors(config.google_embedding_data_path, config.save_dir, config.google_embedding_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Raw_Datapoint(x1='Athens', x2='Greece', x3='Baghdad', y='Iraq', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bangkok', y='Thailand', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Beijing', y='China', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Berlin', y='Germany', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Bern', y='Switzerland', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Cairo', y='Egypt', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Canberra', y='Australia', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Hanoi', y='Vietnam', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Havana', y='Cuba', task='capital-common-countries'),\n Raw_Datapoint(x1='Athens', x2='Greece', x3='Helsinki', y='Finland', task='capital-common-countries')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 46
    }
   ],
   "source": [
    "# Load the google analogies training dataset:\n",
    "analogy_dataset = load_data()\n",
    "analogy_dataset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-88888c737abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Transform the data such that it includes the embeddings of the words in consideration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformed_analogy_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_subspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalogy_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_boluk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Obtaining the dimensionality of the word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_vectors' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'word_vectors' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "# Transform the data such that it includes the embeddings of the words in consideration\n",
    "transformed_analogy_dataset, gender_subspace = transform_data(word_vectors, analogy_dataset, use_boluk = False)\n",
    "\n",
    "\n",
    "# Obtaining the dimensionality of the word embeddings\n",
    "word_embedding_dim = transformed_analogy_dataset[0].gt_embedding.shape[0]\n",
    "\n",
    "# Testing the transformed analogy dataset\n",
    "assert transformed_analogy_dataset[0].analogy_embeddings.shape[0] == word_embedding_dim * 3\n",
    "assert transformed_analogy_dataset[0].gt_embedding.shape[0] == word_embedding_dim\n",
    "assert transformed_analogy_dataset[0].protected.shape[0] == 1\n",
    "\n",
    "print(transformed_analogy_dataset[0].analogy_embeddings.shape)\n",
    "print(transformed_analogy_dataset[0].gt_embedding.shape)\n",
    "print(transformed_analogy_dataset[0].protected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the grid-search and obtain the np.dot(w.T, g) values\n",
    "learning_rate_list = [2 ** -12, 2 ** -6, 2 ** -3]\n",
    "adversary_loss_weight_list = [1.0, 0.5, 0.1]\n",
    "\n",
    "# For the saved model checkpoints pertaining to the word embedding type\n",
    "word_embedding_type = 'GNews'\n",
    "\n",
    "# Performing the grid search\n",
    "utility_functions.grid_search(learning_rate_list, adversary_loss_weight_list, word_embedding_dim, gender_subspace, transformed_analogy_dataset, word_embedding_type, 'models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "gender_subspace = torch.zeros(word_embedding_dim)\n",
    "\n",
    "def load_model(model_path: Path, word_embedding_dim, gender_subspace):\n",
    "    # with open(str(model_path), \"rb\") as f:\n",
    "    #     state_dict = pickle.load(f)\n",
    "    # device = torch.device('cpu')\n",
    "    # print(model_path)\n",
    "    state_dict = torch.load(str(model_path), map_location=torch.device('cpu'))\n",
    "    model = AdversarialDebiasing(\n",
    "                    seed = 42,\n",
    "                    word_embedding_dim = word_embedding_dim,\n",
    "                    num_epochs = 500,\n",
    "                    debias = False,\n",
    "                    gender_subspace = gender_subspace,\n",
    "                    batch_size = 256,\n",
    "                    adversary_loss_weight = 0.1,\n",
    "                    classifier_learning_rate = 2 ** -6,\n",
    "                    adversary_learning_rate = 2 ** -6\n",
    "                )\n",
    "    \n",
    "    model.W1 = state_dict[\"W1\"]\n",
    "    model.W2 = state_dict[\"W2\"]\n",
    "    \n",
    "    return model\n",
    "\n",
    "debiased_models = []\n",
    "non_debiased_models = []\n",
    "\n",
    "\n",
    "ModelResult = namedtuple('ModelResult', ['best_model', 'last_model', 'embedding_type', 'learning_rate', 'adversary_weight', 'debiased'])\n",
    "\n",
    "for model_base_path in [Path('models/debiased'), Path('models/non_debiased')]:\n",
    "    l, debiased = (non_debiased_models, False) if 'non_debiased' in str(model_base_path) else (debiased_models, True)\n",
    "\n",
    "    for model_path in model_base_path.iterdir():\n",
    "        if '_last' in str(model_path):\n",
    "            continue\n",
    "            \n",
    "        m = re.search('^([A-Za-z]+)_([\\d.]+)_([\\d.]+)(_last){0,1}.pckl$', str(model_path.name))\n",
    "        embeddings = m.group(1)\n",
    "        learning_rate = m.group(2)\n",
    "        adversary_weight = m.group(3)\n",
    "        \n",
    "        best_model = load_model(model_path, word_embedding_dim, gender_subspace)\n",
    "        \n",
    "        last_model_path = model_path.parent / f\"{model_path.stem}_last{model_path.suffix}\"\n",
    "        last_model = load_model(last_model_path, word_embedding_dim, gender_subspace)\n",
    "        \n",
    "        \n",
    "        l.append(ModelResult(best_model, last_model, embeddings, learning_rate, adversary_weight, debiased))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e828b4610>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8084b3d0>, embedding_type='GNews', learning_rate='0.125', adversary_weight='1.0', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e828b4ad0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e828081d0>, embedding_type='WikipediaVec', learning_rate='0.125', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80863850>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e808639d0>, embedding_type='WikipediaVec', learning_rate='0.000244140625', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e808636d0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80867bd0>, embedding_type='WikipediaVec', learning_rate='0.125', adversary_weight='1.0', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80867a10>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80867950>, embedding_type='GNews', learning_rate='0.000244140625', adversary_weight='1.0', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827ccb90>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80852390>, embedding_type='WikipediaVec', learning_rate='0.015625', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8278cc10>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8278c610>, embedding_type='Glove', learning_rate='0.000244140625', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e829d1250>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e829ebb50>, embedding_type='WikipediaVec', learning_rate='0.015625', adversary_weight='1.0', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80825f90>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e829d1d50>, embedding_type='GNews', learning_rate='0.015625', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8283d490>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82819990>, embedding_type='GNews', learning_rate='0.000244140625', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827cff90>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82819d90>, embedding_type='Glove', learning_rate='0.000244140625', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827efbd0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827ef210>, embedding_type='WikipediaVec', learning_rate='0.015625', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80833350>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80833f10>, embedding_type='GNews', learning_rate='0.000244140625', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827dbf50>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e808331d0>, embedding_type='GNews', learning_rate='0.125', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827db7d0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80815e10>, embedding_type='GNews', learning_rate='0.015625', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80829510>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e808295d0>, embedding_type='Glove', learning_rate='0.125', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8288d910>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80829f50>, embedding_type='GNews', learning_rate='0.125', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8288dc10>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8288d750>, embedding_type='WikipediaVec', learning_rate='0.125', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827f4150>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827f4610>, embedding_type='Glove', learning_rate='0.015625', adversary_weight='0.5', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827f4510>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8082de90>, embedding_type='WikipediaVec', learning_rate='0.000244140625', adversary_weight='1.0', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8082d750>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8082d850>, embedding_type='Glove', learning_rate='0.125', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8082d3d0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8082ddd0>, embedding_type='Glove', learning_rate='0.015625', adversary_weight='1.0', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807b58d0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807b5710>, embedding_type='Glove', learning_rate='0.015625', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80832cd0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80832850>, embedding_type='Glove', learning_rate='0.000244140625', adversary_weight='1.0', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80832fd0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80832b90>, embedding_type='Glove', learning_rate='0.125', adversary_weight='1.0', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80832a90>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807b9c10>, embedding_type='WikipediaVec', learning_rate='0.000244140625', adversary_weight='0.1', debiased=True), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807b9550>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807b96d0>, embedding_type='GNews', learning_rate='0.015625', adversary_weight='1.0', debiased=True)] 27\n",
      "[ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807b9750>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807b9b50>, embedding_type='GNews', learning_rate='0.125', adversary_weight='1.0', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bdfd0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bded0>, embedding_type='WikipediaVec', learning_rate='0.125', adversary_weight='0.5', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bdb10>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bd790>, embedding_type='WikipediaVec', learning_rate='0.000244140625', adversary_weight='0.5', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bd8d0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bd5d0>, embedding_type='WikipediaVec', learning_rate='0.125', adversary_weight='1.0', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bda90>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bdc90>, embedding_type='GNews', learning_rate='0.000244140625', adversary_weight='1.0', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bdf90>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bd490>, embedding_type='WikipediaVec', learning_rate='0.015625', adversary_weight='0.5', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e807bd710>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8279eb10>, embedding_type='WikipediaVec', learning_rate='0.015625', adversary_weight='1.0', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80835850>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e828bba10>, embedding_type='GNews', learning_rate='0.015625', adversary_weight='0.5', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e827b6910>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80818b50>, embedding_type='GNews', learning_rate='0.000244140625', adversary_weight='0.5', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80818d90>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82783a50>, embedding_type='WikipediaVec', learning_rate='0.015625', adversary_weight='0.1', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80818b10>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80809450>, embedding_type='GNews', learning_rate='0.000244140625', adversary_weight='0.1', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e80809ed0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82897210>, embedding_type='GNews', learning_rate='0.125', adversary_weight='0.5', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e828979d0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82897c10>, embedding_type='GNews', learning_rate='0.015625', adversary_weight='0.1', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82881bd0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82881f10>, embedding_type='GNews', learning_rate='0.125', adversary_weight='0.1', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e828814d0>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82881ed0>, embedding_type='WikipediaVec', learning_rate='0.125', adversary_weight='0.1', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e82881590>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8281c410>, embedding_type='WikipediaVec', learning_rate='0.000244140625', adversary_weight='1.0', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8281ca10>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8281ce90>, embedding_type='WikipediaVec', learning_rate='0.000244140625', adversary_weight='0.1', debiased=False), ModelResult(best_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8281c150>, last_model=<adversarial_debiasing.AdversarialDebiasing object at 0x7f5e8281ce10>, embedding_type='GNews', learning_rate='0.015625', adversary_weight='1.0', debiased=False)] 18\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(debiased_models, len(debiased_models))\n",
    "print(non_debiased_models, len(non_debiased_models))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     0.125  0.015625  0.000244140625\n0.1    0.0       0.0             0.0\n1.0    0.0       0.0             0.0\n0.5    0.0       0.0             0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0.125</th>\n      <th>0.015625</th>\n      <th>0.000244140625</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>0.5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 72
    }
   ],
   "source": [
    "learning_rates = list(set(x.learning_rate for x in debiased_models)) \n",
    "adversary_weights = list(set(x.adversary_weight for x in debiased_models)) \n",
    "\n",
    "\n",
    "box_df = pd.DataFrame([], columns=learning_rates, index=adversary_weights)\n",
    "\n",
    "for model_result in debiased_models:\n",
    "    box_df.loc[model_result.adversary_weight, model_result.learning_rate] = np.dot(model_result.last_model.W1.clone().detach().numpy().T, gender_subspace).item()\n",
    "\n",
    "box_df\n",
    "# debiased_model_best.W1 = best_state_dict[\"W1\"]\n",
    "# debiased_model_best.W2 = best_state_dict[\"W2\"]\n",
    "# \n",
    "# debiased_model_last.W1 = last_state_dict[\"W1\"]\n",
    "# debiased_model_last.W2 = last_state_dict[\"W2\"]\n",
    "# \n",
    "# print(\"Best : {}\".format(np.dot(debiased_model_best.W1.clone().detach().cpu().numpy().T, gender_subspace.T)))\n",
    "# \n",
    "# print(\"Last : {}\".format(np.dot(debiased_model_last.W1.clone().detach().cpu().numpy().T, gender_subspace.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embedding_dim = transformed_analogy_dataset[0].gt_embedding.shape[0]\n",
    "# # Training the variant of the model without debiasing\n",
    "# non_debiased_model = AdversarialDebiasing(\n",
    "#     word_embedding_dim=word_embedding_dim,\n",
    "#     num_epochs=500,\n",
    "#     debias=False,\n",
    "#     gender_subspace=gender_subspace,\n",
    "#     batch_size=256,\n",
    "#     adversary_loss_weight=0.1,\n",
    "#     classifier_learning_rate = 2 ** -6,\n",
    "#     adversary_learning_rate = 2 ** -6\n",
    "# )\n",
    "# non_debiased_model.fit(dataset=transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = non_debiased_model.get_model_weights()\n",
    "print(np.dot(W1.detach().numpy().T,gender_subspace.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_subspace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the variant of the model with debiasing\n",
    "# debiased_model = AdversarialDebiasing(\n",
    "#     word_embedding_dim=word_embedding_dim,\n",
    "#     num_epochs=500,\n",
    "#     debias=True,\n",
    "#     gender_subspace=gender_subspace,\n",
    "#     batch_size=256,\n",
    "#     adversary_loss_weight=0.1,\n",
    "#     classifier_learning_rate = 2 ** -8,\n",
    "#     adversary_learning_rate = 2 ** -8\n",
    "# )\n",
    "debiased_model = AdversarialDebiasing(\n",
    "    word_embedding_dim=word_embedding_dim,\n",
    "    num_epochs=500,\n",
    "    debias=True,\n",
    "    gender_subspace=gender_subspace,\n",
    "    batch_size=256,\n",
    "    adversary_loss_weight=0.1,\n",
    "    classifier_learning_rate = 2 ** -6,\n",
    "    adversary_learning_rate = 2 ** -6\n",
    ")\n",
    "\n",
    "debiased_model.fit(dataset=transformed_analogy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = debiased_model.get_model_weights()\n",
    "print(np.dot(W1.clone().cpu().detach().numpy().T,gender_subspace.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples to test the models upon\n",
    "datapoints, test_analogies = [], []\n",
    "with open(os.path.join('data', 'sexism-traps.txt'), 'r') as f:\n",
    "    # Reading each line\n",
    "    for line in f.readlines():\n",
    "        words = line.split()\n",
    "        if words[0] == ':':\n",
    "            continue\n",
    "        test_analogies.append(words)\n",
    "        word_embeddings = word_vectors[words]\n",
    "        word_embeddings = np.reshape(word_embeddings, (1, -1))\n",
    "        datapoints.append(word_embeddings)\n",
    "datapoints = np.vstack(datapoints)\n",
    "print(datapoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation of the non-debiased model\n",
    "\n",
    "# non_debiased_predictions = non_debiased_model.predict(datapoints)\n",
    "features = torch.cat([torch.Tensor(x).unsqueeze_(0) for x in datapoints])\n",
    "x1 = features[:, 0:word_embedding_dim]\n",
    "x2 = features[:, word_embedding_dim:word_embedding_dim * 2]\n",
    "x3 = features[:, word_embedding_dim * 2:word_embedding_dim * 3]\n",
    "\n",
    "non_debiased_predictions = x2 + x3 - x1\n",
    "non_debiased_predictions = non_debiased_predictions.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_debiased_most_similar_list = utility_functions.obtain_most_similar(non_debiased_predictions, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the similarity list for the non-debiased model\n",
    "non_debiased_most_similar_list_data_frames = []\n",
    "for i in range(len(non_debiased_most_similar_list)):\n",
    "    # print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    temp_data_frame = pd.DataFrame(non_debiased_most_similar_list[i][1:], columns = ['Neighbor', 'Similarity'])\n",
    "    non_debiased_most_similar_list_data_frames.append(temp_data_frame)\n",
    "    # print(tabulate(temp_data_frame, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation of the debiased model\n",
    "debiased_predictions = debiased_model.predict(datapoints)\n",
    "debiased_most_similar_list = utility_functions.obtain_most_similar(debiased_predictions, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the similarity list for the debiased model\n",
    "debiased_most_similar_list_data_frames = []\n",
    "for i in range(len(debiased_most_similar_list)):\n",
    "    # print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    temp_data_frame = pd.DataFrame(debiased_most_similar_list[i][1:], columns = ['Neighbor', 'Similarity'])\n",
    "    debiased_most_similar_list_data_frames.append(temp_data_frame)\n",
    "    # print(tabulate(temp_data_frame, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the dataframes pertaining to both the variants of the model\n",
    "iterables = [['Biased', 'Debiased'], ['Neighbour', 'Similarity']]\n",
    "index = pd.MultiIndex.from_product(iterables)\n",
    "overall_data_frames_list = []\n",
    "for i in range(len(non_debiased_most_similar_list)):\n",
    "    overall_list = []\n",
    "    print(\"{} : {} :: {} : \".format(test_analogies[i][0], test_analogies[i][1], test_analogies[i][2]))\n",
    "    for j in range(1, len(non_debiased_most_similar_list[i])):\n",
    "        temp_list = []\n",
    "        temp_list.append(non_debiased_most_similar_list[i][j][0])\n",
    "        temp_list.append(round(non_debiased_most_similar_list[i][j][1], 3))\n",
    "        temp_list.append(debiased_most_similar_list[i][j][0])\n",
    "        temp_list.append(round(debiased_most_similar_list[i][j][1], 3))\n",
    "        overall_list.append(temp_list)\n",
    "    temp_df = pd.DataFrame(overall_list, columns = index)\n",
    "    # print(temp_df.to_string(index = False))\n",
    "    print(tabulate(temp_df, headers = ['Biased\\nNeighbour', 'Biased\\nSimilarity', 'Debiased\\nNeighbour', 'Debiased\\nSimilarity'], tablefmt = 'psql', showindex = False))\n",
    "    overall_data_frames_list.append(temp_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}